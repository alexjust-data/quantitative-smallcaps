# Estructura de Logging Robusta - Sin PÃ©rdida de InformaciÃ³n

**Fecha:** 2025-10-12
**Principio:** Nunca depender de stdout ni redirecciones shell para logs crÃ­ticos

---

## ğŸ§  Principio General

**Los logs deben escribirse directamente desde Python**, mediante un logger persistente (Loguru) con:

1. âœ… Salida a archivo rotativo (no a consola)
2. âœ… Flush inmediato (sin buffer)
3. âœ… Modo seguro multiproceso (`enqueue=True`)
4. âœ… RotaciÃ³n por tamaÃ±o
5. âœ… Nivel DEBUG con timestamps precisos

---

## ğŸ“‚ Estructura de Logs

```
logs/
â”œâ”€â”€ detect_events/
â”‚   â”œâ”€â”€ detect_events_intraday_20251012_143022.log  â† Principal (TODO)
â”‚   â”œâ”€â”€ heartbeat_20251012.log                      â† Progreso incremental
â”‚   â”œâ”€â”€ batches_20251012.log                        â† ConfirmaciÃ³n de guardado
â”‚   â””â”€â”€ detect_events_intraday_20251012_143022.log.1.zip  â† Rotado/comprimido
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ events_intraday_20251012_completed.json     â† SÃ­mbolos completados
â”œâ”€â”€ heartbeats/
â”‚   â””â”€â”€ events_intraday_20251012_heartbeat.json     â† Estado actual (JSON)
â””â”€â”€ ingestion/
    â””â”€â”€ polygon_ingestion.log                       â† Descargas Polygon
```

---

## ğŸ“‹ Logs en Detalle

### 1. **Log Principal** (`detect_events_intraday_YYYYMMDD_HHMMSS.log`)

**PropÃ³sito:** Registro completo de toda la actividad del proceso

**ConfiguraciÃ³n:**
```python
logger.add(
    main_log,
    level="DEBUG",
    format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {process.name}:{thread.name} | {file}:{line} | {message}",
    enqueue=True,        # Multiproceso seguro
    backtrace=True,      # Traceback completo en excepciones
    diagnose=True,       # Contexto adicional en errores
    rotation="50 MB",    # Rota cada 50 MB
    retention="7 days",  # Mantiene logs 7 dÃ­as
    compression="zip",   # Comprime logs viejos
    mode="a"             # Append si relanzas (resume)
)
```

**Contenido:**
- Todos los niveles: DEBUG, INFO, WARNING, ERROR
- Excepciones con traceback completo
- InformaciÃ³n de proceso y thread
- Archivo y lÃ­nea de cÃ³digo

**Ejemplo:**
```
2025-10-12 14:35:22.123 | INFO     | MainProcess:MainThread | detect_events_intraday.py:867 | [HEARTBEAT] AAPL | batch=5/40 (12.5%) | events=12,450 | RAM=3.24GB
2025-10-12 14:35:22.456 | DEBUG    | MainProcess:MainThread | detect_events_intraday.py:1076 | [RESOURCE] RAM used: 25.3% (3.24/12.8 GB) | CPU: 45.2%
2025-10-12 14:40:15.789 | SUCCESS  | MainProcess:MainThread | detect_events_intraday.py:1061 | [BATCH SAVED] #005 | symbols=50 | events=4,523 | file=events_intraday_20251012_shard0005.parquet | RAM=3.45GB
```

---

### 2. **Heartbeat Log** (`heartbeat_YYYYMMDD.log`)

**PropÃ³sito:** Progreso incremental para saber exactamente dÃ³nde se detuvo

**ConfiguraciÃ³n:**
```python
with open(heartbeat_file, "a", encoding="utf-8", buffering=1) as f:
    f.write(f"{timestamp}\t{symbol}\t{batch_num}\t{total_batches}\t{events_count}\t{mem_gb:.2f}\n")
```

**CaracterÃ­sticas clave:**
- âœ… **Sin buffer** (`buffering=1`): cada lÃ­nea se escribe inmediatamente
- âœ… Formato TSV: fÃ¡cil de parsear con pandas/polars
- âœ… 1 lÃ­nea por sÃ­mbolo procesado
- âœ… Permite reanudar: leer Ãºltima lÃ­nea = Ãºltimo sÃ­mbolo completado

**Formato:**
```
timestamp                 symbol  batch_num  total_batches  events_count  mem_gb
2025-10-12 14:35:22.123  AAPL    5          40             12450         3.24
2025-10-12 14:35:25.456  MSFT    5          40             12523         3.26
2025-10-12 14:35:28.789  TSLA    5          40             12678         3.28
```

**Uso para Resume:**
```python
# Leer Ãºltimo sÃ­mbolo procesado
df = pl.read_csv("logs/detect_events/heartbeat_20251012.log", separator="\t", has_header=False)
last_symbol = df[-1, 1]  # Segunda columna
print(f"Last processed: {last_symbol}")
```

---

### 3. **Batch Log** (`batches_YYYYMMDD.log`)

**PropÃ³sito:** ConfirmaciÃ³n de cada batch guardado (shard completado)

**ConfiguraciÃ³n:**
```python
with open(batch_file, "a", encoding="utf-8", buffering=1) as f:
    f.write(f"{timestamp}\t{batch_num}\t{symbols_count}\t{total_events}\t{shard_file}\t{mem_gb:.2f}\n")
```

**CaracterÃ­sticas clave:**
- âœ… Sin buffer: confirmaciÃ³n inmediata
- âœ… Formato TSV
- âœ… 1 lÃ­nea por batch completado
- âœ… Permite verificar quÃ© shards existen sin leer archivos parquet

**Formato:**
```
timestamp                 batch_num  symbols_count  total_events  shard_file                                     mem_gb
2025-10-12 14:40:15.234  5          50             4523          events_intraday_20251012_shard0005.parquet    3.45
2025-10-12 14:52:30.567  6          50             4678          events_intraday_20251012_shard0006.parquet    3.52
```

**Uso para VerificaciÃ³n:**
```python
# Verificar batches completados vs shards existentes
df = pl.read_csv("logs/detect_events/batches_20251012.log", separator="\t", has_header=False)
completed_batches = df.select(pl.col("column_1").alias("batch_num")).unique()

shards = list(Path("processed/events/shards").glob("events_intraday_20251012_shard*.parquet"))
print(f"Batches logged: {len(completed_batches)}")
print(f"Shards on disk: {len(shards)}")
```

---

### 4. **Heartbeat JSON** (`events_intraday_YYYYMMDD_heartbeat.json`)

**PropÃ³sito:** Estado actual en formato estructurado (JSON)

**ActualizaciÃ³n:** Cada sÃ­mbolo procesado (sobrescribe archivo)

**Contenido:**
```json
{
  "run_id": "events_intraday_20251012",
  "last_symbol": "AAPL",
  "last_timestamp": "2025-10-12T14:35:22.123456",
  "batch_num": 5,
  "total_batches": 40,
  "progress_pct": 12.5,
  "events_detected": 12450,
  "memory_gb": 3.24,
  "status": "running"
}
```

**Uso para Monitoreo:**
```python
# Script de monitoreo
import json
import time

while True:
    with open("logs/heartbeats/events_intraday_20251012_heartbeat.json") as f:
        data = json.load(f)

    print(f"\rProgress: {data['progress_pct']:.1f}% | Symbol: {data['last_symbol']} | RAM: {data['memory_gb']:.2f}GB", end="")
    time.sleep(5)
```

---

### 5. **Checkpoint JSON** (`events_intraday_YYYYMMDD_completed.json`)

**PropÃ³sito:** Lista de sÃ­mbolos completados (para resume)

**ActualizaciÃ³n:** Cada batch completado

**Contenido:**
```json
{
  "run_id": "events_intraday_20251012",
  "completed_symbols": [
    "AAPL",
    "MSFT",
    "TSLA",
    ...
  ],
  "total_completed": 250,
  "last_updated": "2025-10-12T14:40:15.234567"
}
```

**Uso para Resume:**
```python
# Cargar checkpoint
with open("logs/checkpoints/events_intraday_20251012_completed.json") as f:
    checkpoint = json.load(f)

completed = set(checkpoint["completed_symbols"])
all_symbols = load_symbols()
remaining = [s for s in all_symbols if s not in completed]

print(f"Completed: {len(completed)}")
print(f"Remaining: {len(remaining)}")
```

---

## ğŸ” Comandos de Monitoreo

### **Ver Progreso en Vivo (Heartbeat Log)**

```powershell
# PowerShell - Ãºltimas 10 lÃ­neas, actualizaciÃ³n en vivo
Get-Content logs\detect_events\heartbeat_20251012.log -Tail 10 -Wait
```

```bash
# Bash/WSL - Ãºltimas 10 lÃ­neas, actualizaciÃ³n en vivo
tail -f -n 10 logs/detect_events/heartbeat_20251012.log
```

### **Ver Batches Completados (Batch Log)**

```powershell
Get-Content logs\detect_events\batches_20251012.log -Tail 5 -Wait
```

### **Ver Log Principal (Debug Completo)**

```powershell
Get-Content logs\detect_events\detect_events_intraday_*.log -Tail 50 -Wait
```

### **Analizar Heartbeat (Pandas)**

```python
import pandas as pd

# Leer heartbeat log
df = pd.read_csv(
    "logs/detect_events/heartbeat_20251012.log",
    sep="\t",
    names=["timestamp", "symbol", "batch_num", "total_batches", "events_count", "mem_gb"]
)

# EstadÃ­sticas
print(f"Total symbols processed: {len(df)}")
print(f"Average events per symbol: {df['events_count'].diff().mean():.0f}")
print(f"Peak memory: {df['mem_gb'].max():.2f} GB")
print(f"Last symbol: {df.iloc[-1]['symbol']}")

# Progreso por batch
df.groupby("batch_num")["symbol"].count().plot(kind="bar", title="Symbols per Batch")
```

---

## ğŸš¨ DetecciÃ³n de Fallos

### **Si el proceso se detiene sin aviso:**

1. **Ver Ãºltima lÃ­nea del heartbeat log:**
   ```powershell
   Get-Content logs\detect_events\heartbeat_20251012.log | Select-Object -Last 1
   ```

   **Output:** `2025-10-12 14:35:28.789	TSLA	5	40	12678	3.28`

   **InterpretaciÃ³n:** El proceso se detuvo mientras procesaba `TSLA` en el batch #5

2. **Ver Ãºltimo batch guardado:**
   ```powershell
   Get-Content logs\detect_events\batches_20251012.log | Select-Object -Last 1
   ```

   **Output:** `2025-10-12 14:30:15.234	4	50	4123	events_intraday_20251012_shard0004.parquet	3.15`

   **InterpretaciÃ³n:** El Ãºltimo batch completo fue #4, el batch #5 estaba en progreso

3. **Ver checkpoint:**
   ```powershell
   Get-Content logs\checkpoints\events_intraday_20251012_completed.json
   ```

   **InterpretaciÃ³n:** Lista de sÃ­mbolos completados hasta el Ãºltimo checkpoint

4. **Relanzar con --resume:**
   ```powershell
   python -u scripts\processing\detect_events_intraday.py --from-file ... --resume
   ```

   **Â¿QuÃ© hace?**
   - Lee checkpoint
   - Salta todos los sÃ­mbolos completados
   - ContinÃºa desde el siguiente sÃ­mbolo pendiente

---

## ğŸ’¡ Por QuÃ© Esta Estructura Funciona

| Problema | SoluciÃ³n |
|----------|----------|
| **Windows mata proceso sin aviso** | Logger escribe directo a archivo (no stdout) |
| **No sÃ© dÃ³nde se detuvo** | Heartbeat log (1 lÃ­nea por sÃ­mbolo, sin buffer) |
| **PerdÃ­ 2 horas de trabajo** | Batching + shards guardados inmediatamente |
| **No sÃ© si el batch se guardÃ³** | Batch log confirma cada shard |
| **No puedo monitorear sin abrir logs** | Heartbeat JSON con estado actual |
| **Resume no funciona** | Checkpoint con lista completa de sÃ­mbolos |
| **Logs crecen sin control** | RotaciÃ³n a 50 MB + compresiÃ³n ZIP |

---

## ğŸ“Š AnÃ¡lisis Post-Mortem

Si el proceso falla, puedes analizar los logs para entender quÃ© pasÃ³:

```python
import polars as pl
from pathlib import Path

# Cargar heartbeat log
heartbeat = pl.read_csv(
    "logs/detect_events/heartbeat_20251012.log",
    separator="\t",
    has_header=False,
    new_columns=["timestamp", "symbol", "batch_num", "total_batches", "events_count", "mem_gb"]
)

# Cargar batch log
batches = pl.read_csv(
    "logs/detect_events/batches_20251012.log",
    separator="\t",
    has_header=False,
    new_columns=["timestamp", "batch_num", "symbols_count", "total_events", "shard_file", "mem_gb"]
)

# AnÃ¡lisis
print("="*60)
print("POST-MORTEM ANALYSIS")
print("="*60)

print(f"\nTotal symbols processed: {len(heartbeat)}")
print(f"Total batches completed: {len(batches)}")

last_heartbeat = heartbeat[-1]
print(f"\nLast symbol processed: {last_heartbeat['symbol'][0]}")
print(f"Last batch: {last_heartbeat['batch_num'][0]}/{last_heartbeat['total_batches'][0]}")
print(f"Memory at failure: {last_heartbeat['mem_gb'][0]:.2f} GB")

# Â¿CuÃ¡l fue el Ãºltimo batch completado?
if len(batches) > 0:
    last_batch = batches[-1]
    print(f"\nLast batch saved: #{last_batch['batch_num'][0]:03d}")
    print(f"Shard file: {last_batch['shard_file'][0]}")
    print(f"Events in batch: {last_batch['total_events'][0]:,}")

# Memoria usage trend
print(f"\nMemory usage trend:")
print(f"  Min: {heartbeat['mem_gb'].min():.2f} GB")
print(f"  Max: {heartbeat['mem_gb'].max():.2f} GB")
print(f"  Avg: {heartbeat['mem_gb'].mean():.2f} GB")

# Â¿Hay fuga de memoria?
mem_trend = heartbeat.select(["batch_num", "mem_gb"]).group_by("batch_num").agg(pl.col("mem_gb").max())
if mem_trend["mem_gb"].is_sorted():
    print("\nâš ï¸  WARNING: Memory is monotonically increasing (possible leak)")
```

---

## ğŸ¯ Checklist Final

- âœ… Logger principal con rotaciÃ³n y compresiÃ³n
- âœ… Heartbeat log sin buffer (TSV)
- âœ… Batch log sin buffer (TSV)
- âœ… Heartbeat JSON para monitoreo
- âœ… Checkpoint JSON para resume
- âœ… Nivel DEBUG con timestamps precisos
- âœ… Traceback completo en excepciones
- âœ… Modo append para resume seguro
- âœ… Thread-safe con `enqueue=True`
- âœ… Sin dependencia de stdout ni redirecciones

---

**Con esta estructura, NUNCA perderÃ¡s informaciÃ³n, incluso si Windows mata el proceso sin previo aviso.**
