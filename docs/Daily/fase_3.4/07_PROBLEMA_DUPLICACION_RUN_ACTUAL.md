# ğŸš¨ PROBLEMA CRÃTICO: Run 20251014 ESTÃ DUPLICANDO

**Fecha detecciÃ³n:** 2025-10-14 17:45 UTC
**Severidad:** ğŸ”´ CRÃTICA
**Estado:** âš ï¸ PROCESO ACTIVO GENERANDO DUPLICADOS

---

## â“ TUS 3 PREGUNTAS - RESPUESTAS

### **1. Â¿El proceso actual estÃ¡ duplicando sÃ­mbolos?**

**SÃ** âœ—

**Evidencia:**
```
SÃ­mbolo: OPRX
  - Eventos Ãºnicos: 241
  - Aparece en: 3 shards
  - Total eventos: 723 (3 copias Ã— 241)
  - Duplicados: 482

!!! DUPLICACIÃ“N ACTIVA DETECTADA !!!
```

**Todos los 120 sÃ­mbolos en shards estÃ¡n duplicados en 3 shards cada uno.**

---

### **2. Â¿Identifica cuando un sÃ­mbolo ya estÃ¡ acabado?**

**SÃ, pero NO FUNCIONA CORRECTAMENTE** âœ—

**Checkpoint actual:**
```json
{
  "run_id": "events_intraday_20251014",
  "completed_symbols": 186 sÃ­mbolos,
  "total_completed": 186,
  "last_updated": "2025-10-14T17:19:37"
}
```

**El checkpoint SÃ guarda sÃ­mbolos completados, PERO:**
- âŒ Hay 3 workers procesando sÃ­mbolos simultÃ¡neamente
- âŒ Los workers NO consultan el checkpoint antes de procesar
- âŒ Los 3 workers procesan los MISMOS sÃ­mbolos
- âŒ Resultado: Cada sÃ­mbolo se procesa 3 veces (1 por worker)

**Discrepancia:**
- Checkpoint: 186 sÃ­mbolos "completados"
- Shards: 120 sÃ­mbolos Ãºnicos
- Diferencia: 66 sÃ­mbolos solo en checkpoint (sin shards)

**Esto confirma:** Los workers guardan en checkpoint PERO siguen procesando sÃ­mbolos ya completados.

---

### **3. Â¿CuÃ¡les son los archivos generados para la BBDD?**

**Archivos FÃSICOS generados:**

```
processed/events/shards/
â”œâ”€â”€ events_intraday_20251014_shard0000.parquet (233 KB)
â”œâ”€â”€ events_intraday_20251014_shard0001.parquet (170 KB)
â”œâ”€â”€ events_intraday_20251014_shard0002.parquet (173 KB)
â”œâ”€â”€ ...
â”œâ”€â”€ events_intraday_20251014_shard0027.parquet (136 KB)
â””â”€â”€ events_intraday_20251014_shard0028.parquet (119 KB)

Total: 29 shards
TamaÃ±o: 4.58 MB
Eventos: 115,416 (CON DUPLICACIÃ“N)
SÃ­mbolos Ãºnicos: 120
```

**Archivos de CONTROL:**

```
logs/checkpoints/
â””â”€â”€ events_intraday_20251014_completed.json

processed/events/manifests/
â”œâ”€â”€ events_intraday_20251014_shard0000.json
â”œâ”€â”€ events_intraday_20251014_shard0001.json
â”œâ”€â”€ ...
â””â”€â”€ events_intraday_20251014_shard0028.json
```

**Archivos FINALES (cuando termine):**
```
processed/events/
â””â”€â”€ events_intraday_20251014.parquet  (merge de shards, PENDIENTE)
```

**PERO ATENCIÃ“N:** Estos archivos contienen DUPLICACIÃ“N. NO son vÃ¡lidos para la BBDD hasta deduplicar.

---

## ğŸ” CAUSA RAÃZ DEL PROBLEMA

### **Â¿Por quÃ© estÃ¡ duplicando si aplicamos el "fix"?**

**El "fix" de commit `2a7a745` incluÃ­a:**
- âœ… File locks para shard numbering atÃ³mico
- âœ… Manifests por shard
- âœ… ReconciliaciÃ³n con manifests

**PERO EL PROBLEMA ES:**

El orchestrator lanzÃ³ **3 WORKERS SIMULTÃNEOS** a las 16:23:

```
Worker 1: PID=84076
Worker 2: PID=85708
Worker 3: PID=21396
```

**Cada worker:**
1. Lee la lista completa de sÃ­mbolos (1,996)
2. Lee checkpoint (53 completados en ese momento)
3. Calcula sÃ­mbolos pendientes (1,996 - 53 = 1,943)
4. **Los 3 workers procesan los MISMOS 1,943 sÃ­mbolos**

**No hay coordinaciÃ³n entre workers para dividir sÃ­mbolos.**

**Resultado:**
- Worker 1 procesa OPRX â†’ genera shard0000
- Worker 2 procesa OPRX â†’ genera shard0012
- Worker 3 procesa OPRX â†’ genera shard0024

**3 copias del mismo sÃ­mbolo en 3 shards diferentes.**

---

## ğŸ“Š ANÃLISIS DETALLADO

### **Evidencia de DuplicaciÃ³n**

**Todos los 120 sÃ­mbolos en shards aparecen en 3 shards cada uno:**

```
OPRX: 3 shards (shard0000, shard0012, shard0024)
OPTT: 3 shards
ONCY: 3 shards
OR: 3 shards
OPRT: 3 shards
OPEN: 3 shards
... (todos los 120 sÃ­mbolos)
```

**Pattern:**
- Worker 1: shards 0, 3, 6, 9, 12, 15, 18, 21, 24, 27
- Worker 2: shards 1, 4, 7, 10, 13, 16, 19, 22, 25, 28
- Worker 3: shards 2, 5, 8, 11, 14, 17, 20, 23, 26

**Cada worker genera ~10 shards, pero TODOS procesan los MISMOS sÃ­mbolos.**

---

### **Timeline del Run 20251014**

```
04:01 AM - Inicio (primeros shards generados)
06:28 AM - Orchestrator se detuvo (mÃ¡ximo runtime)
06:30 AM - Workers killed
...
16:23 PM - Orchestrator RE-LANZADO
16:26 PM - 3 workers activos
17:19 PM - Ãšltimo checkpoint guardado (186 sÃ­mbolos)
17:45 PM - PROBLEMA DETECTADO
```

**ObservaciÃ³n:** El orchestrator se detuvo y relanzÃ³. Esto puede explicar parte de la duplicaciÃ³n.

---

## âš ï¸ IMPACTO

### **Datos Actuales NO SON VÃLIDOS**

```
Run 20251014:
  Shards: 29 archivos
  Eventos BRUTOS: 115,416
  SÃ­mbolos Ãºnicos: 120

  DuplicaciÃ³n estimada: 66.7% (2 de cada 3 eventos son duplicados)
  Eventos ÃšNICOS estimados: 38,472 (115,416 / 3)
```

**Si el proceso continÃºa asÃ­:**
- 1,996 sÃ­mbolos Ã— 3 workers = 5,988 procesos de sÃ­mbolos
- ~1.5M eventos Ã— 3 = ~4.5M eventos brutos (con 66% duplicaciÃ³n)
- TamaÃ±o final: ~150 MB brutos â†’ ~50 MB deduplicados

---

## ğŸš« PROCESOS ACTIVOS

**Orchestrator:** Ultra Robust Orchestrator
- Archivo: `logs/ultra_robust/orchestrator_20251014.log`
- Inicio: 16:23:31
- Estado: Desconocido (no hay proceso `ps aux`)

**Workers detectados en logs:**
```
Worker 1: logs/ultra_robust/worker_1_20251014_162630.log (5.7 MB)
Worker 2: logs/ultra_robust/worker_2_20251014_162707.log (5.9 MB)
Worker 3: logs/ultra_robust/worker_3_20251014_162811.log (5.5 MB)
```

**Ãšltima actividad:** 17:19 PM (heartbeat)

**Estado actual:** Probablemente DETENIDO (no vimos proceso en `ps aux`)

---

## ğŸ›‘ ACCIÃ“N REQUERIDA INMEDIATA

### **OPCIÃ“N 1: Detener TODO** (RECOMENDADO)

```bash
# 1. Matar cualquier proceso residual
ps aux | grep -E "(ultra_robust|detect_events)" | awk '{print $2}' | xargs kill -9

# 2. Verificar limpieza
ps aux | grep -E "(ultra_robust|detect_events)" | grep -v grep
```

**Resultado esperado:** No procesos activos

---

### **OPCIÃ“N 2: Archivar run corrupto y limpiar**

```bash
# 1. Archivar shards corruptos
mkdir -p archive/corrupted_fase25_20251014
mv processed/events/shards/events_intraday_20251014_shard*.parquet \
   archive/corrupted_fase25_20251014/

# 2. Archivar checkpoint
mv logs/checkpoints/events_intraday_20251014_completed.json \
   archive/corrupted_fase25_20251014/

# 3. Archivar logs
mv logs/ultra_robust/*20251014*.log \
   archive/corrupted_fase25_20251014/
```

---

### **OPCIÃ“N 3: Decidir estrategia**

**A. Usar datos deduplicados existentes (405K eventos)**
- âœ… Ya validados (100% consistencia)
- âœ… Listos YA
- âœ… Podemos lanzar FASE 3.2 HOY
- âš ï¸ Solo 1,133 sÃ­mbolos (no 1,996)

**B. Esperar y arreglar run limpio**
- âŒ Run 20251014 estÃ¡ CORRUPTO
- âŒ Necesita reinicio completo
- â³ 11 dÃ­as para completar 1,996 sÃ­mbolos
- âœ… Dataset 100% limpio desde origen

**C. Deduplicar run 20251014 cuando termine**
- â³ Esperar que termine (si sigue corriendo)
- âœ… Deduplicar eventos (dividir por 3)
- âš ï¸ Misma situaciÃ³n que run 20251013 (validar consistencia)

---

## ğŸ¯ RECOMENDACIÃ“N

**INMEDIATO:**
1. Detener todos los procesos
2. Archivar run 20251014 corrupto
3. Usar datos deduplicados existentes (405K eventos) para FASE 3.2

**Razones:**
- âœ… Datos deduplicados ya validados (100% consistencia)
- âœ… No perdemos mÃ¡s tiempo
- âœ… Podemos lanzar FASE 3.2 HOY
- âœ… Tenemos 1,133 sÃ­mbolos procesados (suficiente)

**OPCIONAL (en paralelo):**
- Arreglar orchestrator para dividir sÃ­mbolos correctamente entre workers
- Re-lanzar FASE 2.5 limpia cuando tengamos tiempo
- Usar como dataset alternativo

---

## ğŸ“‹ ARCHIVOS PARA BBDD (VÃ¡lidos)

**NO usar:**
```
âŒ processed/events/shards/events_intraday_20251014_shard*.parquet
   (CORRUPTOS - con duplicaciÃ³n 66%)
```

**SÃ usar:**
```
âœ… processed/events/events_intraday_enriched_dedup_20251014_101439.parquet
   - 405,886 eventos ÃšNICOS
   - 1,133 sÃ­mbolos
   - Validado 100%

âœ… processed/events/manifest_core_20251014.parquet
   - 10,000 eventos seleccionados
   - Listo para FASE 3.2
```

---

**Autor:** Claude Code
**Fecha:** 2025-10-14 17:45 UTC
**Prioridad:** ğŸ”´ CRÃTICA - AcciÃ³n inmediata requerida
