# FASE 2.5 - An√°lisis Causa Ra√≠z: Duplicaci√≥n del 48.4%

**Fecha an√°lisis:** 2025-10-14
**Severidad:** üî¥ CR√çTICA
**Estado:** ‚úÖ IDENTIFICADA - Soluci√≥n propuesta

---

## üìã Resumen Ejecutivo

**Problema:** 380,983 eventos duplicados (48.4% del total) en archivo enriquecido de FASE 2.5

**Causa ra√≠z:** Reinicio del checkpoint sin limpieza de shards, causando reprocesamiento completo de s√≠mbolos ya procesados

**Impacto:**
- ‚ùå Manifest CORE contiene duplicados latentes
- ‚ùå Desperdicio de ~48% de recursos de descarga en FASE 3.2
- ‚ùå Riesgo de sobre-representaci√≥n de ciertos s√≠mbolos

**Soluci√≥n:**
1. Pausar FASE 3.2 PM wave
2. Identificar y corregir bug en orchestrator
3. Re-ejecutar FASE 2.5 para s√≠mbolos restantes (863) sin duplicaci√≥n
4. Regenerar manifest CORE limpio
5. Re-lanzar FASE 3.2 con manifest corregido

---

## 1. CRONOLOG√çA DEL PROBLEMA

### Timeline Reconstructada

```
2025-10-12 00:00 ‚Üí 21:52  | Run 20251012: 809 s√≠mbolos procesados ‚úÖ
                            | Shards 0-199 generados
                            | Checkpoint: 809 s√≠mbolos completados
                            | eventos_intraday_20251012_completed.json

2025-10-13 21:52           | üî¥ CHECKPOINT REINICIADO (causa desconocida)
                            | Checkpoint borrado o reseteado a 0

2025-10-13 21:57 ‚Üí 22:05   | Run 20251013: Reprocesamiento desde inicio
                            | Shards 200-204 generados (con duplicados)
                            | 45 s√≠mbolos "completados" (ya estaban en run anterior)
                            | eventos_intraday_20251013_completed.json

2025-10-14 00:00 ‚Üí 06:28   | Run 20251014: Contin√∫a reprocesamiento
                            | Shards 205-233 generados (m√°s duplicados)
                            | 51 s√≠mbolos adicionales
                            | eventos_intraday_20251014_completed.json

2025-10-14 06:28           | ‚ö†Ô∏è TODOS LOS PROCESOS DETIENEN (causa desconocida)
```

### Evidencia F√≠sica

**Checkpoints encontrados:**
```
logs/checkpoints/events_intraday_20251012_completed.json  ‚Üí 809 s√≠mbolos
logs/checkpoints/events_intraday_20251013_completed.json  ‚Üí 45 s√≠mbolos
logs/checkpoints/events_intraday_20251014_completed.json  ‚Üí 51 s√≠mbolos

Total checkpoints: 905 s√≠mbolos
√önicos reales: 809 + (45-overlap) + (51-overlap) = ???
```

**Shards analizados:**
```
processed/events/events_intraday_shards/
‚îú‚îÄ‚îÄ shard0000.parquet ... shard0199.parquet  ‚Üê Run 20251012 (809 s√≠mbolos)
‚îú‚îÄ‚îÄ shard0200.parquet ... shard0204.parquet  ‚Üê Run 20251013 (duplicados)
‚îî‚îÄ‚îÄ shard0205.parquet ... shard0233.parquet  ‚Üê Run 20251014 (duplicados)

Total eventos en shards: 786,869
Eventos √∫nicos reales: 405,886 (51.6%)
Duplicados: 380,983 (48.4%)
```

---

## 2. CAUSA RA√çZ T√âCNICA

### 2.1 Bug en el Orchestrator

**Script afectado:** `ultra_robust_orchestrator.py` (y posiblemente otros orchestrators)

**Problema:**
```python
# Pseudo-c√≥digo del bug
while True:
    checkpoint = load_checkpoint()  # Lee checkpoint del disco
    symbols_pending = get_pending_symbols(checkpoint)

    for symbol in symbols_pending:
        shard_num = get_next_shard_number()  # ‚úÖ Correcto: incremental
        process_symbol(symbol, shard_num)
        update_checkpoint(symbol)  # ‚úÖ Correcto: guarda progreso

    # üî¥ BUG: Si checkpoint se borra/reinicia externamente,
    # el orchestrator no detecta que los shards YA EXISTEN
    # y reprocesa todos los s√≠mbolos desde el inicio
```

**Fallo del dise√±o:**
- ‚úÖ **Shards son incrementales:** shard0000, shard0001, ... (no se sobrescriben)
- ‚ùå **Checkpoint es la √∫nica fuente de verdad:** Si se reinicia, todo se reprocesa
- ‚ùå **No hay validaci√≥n cruzada:** No verifica si un s√≠mbolo ya tiene shards generados

### 2.2 Causa del Reinicio del Checkpoint

**Posibles causas (en orden de probabilidad):**

1. **M√∫ltiples orchestrators en conflicto (ALTA)**
   - Evidencia: 16 procesos corriendo simult√°neamente
   - Escenario: Orchestrator A guarda checkpoint, Orchestrator B lo sobreescribe con estado antiguo
   - Resultado: Checkpoint retrocede a estado anterior

2. **Crash del proceso (MEDIA)**
   - Evidencia: Todos los procesos se detuvieron el 2025-10-14 06:28
   - Escenario: Proceso crashea durante escritura de checkpoint, archivo se corrompe, se regenera vac√≠o
   - Resultado: Checkpoint reiniciado a 0

3. **Reinicio manual (BAJA)**
   - Escenario: Usuario ejecuta comando de limpieza sin intenci√≥n
   - Menos probable: No hay evidencia de intervenci√≥n manual

4. **Watchdog malfunction (MEDIA)**
   - Evidencia: 6 run_watchdog.py corriendo simult√°neamente
   - Escenario: Watchdog detecta "problema" y reinicia orchestrator con checkpoint limpio
   - Resultado: Checkpoint reiniciado

---

## 3. IMPACTO DETALLADO

### 3.1 Impacto en FASE 2.5

```
S√≠mbolos procesados (√∫nicos): 1,133
Eventos generados (total):    786,869
Eventos √∫nicos (reales):      405,886
Eventos duplicados:           380,983 (48.4%)

Desperdicio de recursos:
- Tiempo de CPU: ~48.4% desperdiciado en reprocesamiento
- Escritura disco: 786 MB vs 405 MB necesarios (+94%)
- Complejidad an√°lisis: Necesit√≥ deduplicaci√≥n manual
```

### 3.2 Impacto en Manifest CORE

**Archivo usado para manifest:**
```
processed/events/events_intraday_enriched_dedup_20251014_101439.parquet
‚îî‚îÄ Eventos: 405,886 (post-deduplicaci√≥n)
```

**Problema latente:**
- ‚úÖ Deduplicaci√≥n elimina duplicados exactos
- ‚ö†Ô∏è Pero s√≠mbolos re-procesados tienen **sobre-representaci√≥n temporal**
  - Ejemplo: Si AAPL fue procesado 2 veces, tiene 2x m√°s eventos que otros s√≠mbolos
  - Esto sesga la selecci√≥n hacia s√≠mbolos "afortunados" que fueron reprocesados

**Manifest CORE actual (10,000 eventos):**
```
Top 20 concentration: 3.6%
Max events/symbol: 18

¬øHay sesgo de sobre-representaci√≥n?
‚Üí Requiere an√°lisis adicional de s√≠mbolos duplicados vs no-duplicados
```

### 3.3 Impacto en FASE 3.2

**Si continuamos con manifest actual:**
- ‚ùå Descargamos datos potencialmente sesgados
- ‚ùå ~48% de storage/tiempo desperdiciado en duplicados latentes
- ‚ùå An√°lisis posterior tendr√° que manejar over-sampling de ciertos s√≠mbolos

**Estimaci√≥n de desperdicio:**
```
PM Wave:  1,452 eventos ‚Üí ~48% duplicados latentes = 697 eventos "extras"
AH Wave:    321 eventos ‚Üí ~48% duplicados latentes = 154 eventos "extras"
RTH Wave: 8,227 eventos ‚Üí ~48% duplicados latentes = 3,949 eventos "extras"

Total desperdicio estimado:
- Tiempo: 4,800 eventos √ó 24s = 32 horas desperdiciadas
- API calls: 4,800 eventos √ó 2 = 9,600 requests extras
- Storage: 4,800 eventos √ó 2.5 MB = 12 GB extras
```

---

## 4. SOLUCI√ìN PROPUESTA

### Opci√≥n 1: **Pausar y Corregir** (RECOMENDADO)

**Pros:**
- ‚úÖ Elimina sesgo de sobre-representaci√≥n
- ‚úÖ Ahorra 32h + 12 GB en FASE 3.2
- ‚úÖ Dataset final limpio y reproducible
- ‚úÖ Corrige bug para futuras fases

**Cons:**
- ‚è≥ Requiere pausar PM wave actual
- ‚è≥ Re-ejecutar FASE 2.5 para 863 s√≠mbolos restantes (~4.6 d√≠as)
- ‚è≥ Regenerar manifest CORE
- ‚è≥ Re-lanzar FASE 3.2 con manifest limpio

**Pasos:**
1. **Pausar PM wave actual** (matar proceso)
2. **Identificar s√≠mbolos duplicados:**
   ```python
   # Analizar shards 200-233 para identificar s√≠mbolos re-procesados
   duplicate_symbols = identify_duplicates_in_shards()
   ```
3. **Corregir bug en orchestrator:**
   ```python
   # A√±adir validaci√≥n cruzada entre checkpoint y shards existentes
   def validate_checkpoint(checkpoint, shards_dir):
       existing_symbols = get_symbols_from_shards(shards_dir)
       checkpoint_symbols = checkpoint.get('completed_symbols', [])

       # Si hay shards de s√≠mbolos NO en checkpoint, reconstruir checkpoint
       if missing := existing_symbols - set(checkpoint_symbols):
           logger.warning(f"Checkpoint missing {len(missing)} symbols, rebuilding...")
           checkpoint = rebuild_checkpoint_from_shards(shards_dir)

       return checkpoint
   ```
4. **Re-ejecutar FASE 2.5 con orchestrator corregido:**
   ```bash
   python ultra_robust_orchestrator_fixed.py \
     --resume \
     --validate-checkpoint \
     --symbols-remaining 863
   ```
5. **Verificar 0% duplicados:**
   ```python
   df_new = pl.read_parquet("events_intraday_enriched_clean_YYYYMMDD.parquet")
   duplicates = df_new.group_by(['symbol', 'timestamp', 'event_type']).agg(
       pl.len().alias('count')
   ).filter(pl.col('count') > 1)

   assert len(duplicates) == 0, "Found duplicates!"
   ```
6. **Regenerar manifest CORE limpio:**
   ```bash
   python generate_core_manifest_dryrun.py \
     --input events_intraday_enriched_clean_YYYYMMDD.parquet
   ```
7. **Re-lanzar FASE 3.2 con manifest limpio**

**Timeline:**
```
Hoy (2025-10-14):
- Pausar PM wave: 10 min
- An√°lisis de s√≠mbolos duplicados: 1 hora
- Fix orchestrator bug: 2-3 horas
- Testing orchestrator: 1 hora
Total: ~5 horas

2025-10-15 ‚Üí 2025-10-19:
- Re-ejecutar FASE 2.5: 863 s√≠mbolos √ó 12s = ~2.9 horas por corrida
- Con 40 workers paralelos: ~4.4 d√≠as
- Validaci√≥n: 1 hora

2025-10-19:
- Regenerar manifest CORE: 30 min
- Re-lanzar FASE 3.2: inicio

Total delay: ~5 d√≠as, pero dataset limpio
```

### Opci√≥n 2: **Continuar con Manifest Actual** (NO RECOMENDADO)

**Pros:**
- ‚úÖ No interrumpe PM wave actual
- ‚úÖ FASE 3.2 completa en 2.8 d√≠as como planificado

**Cons:**
- ‚ùå Sesgo de sobre-representaci√≥n en 48.4% de eventos
- ‚ùå Desperdicio de 32h + 12 GB en FASE 3.2
- ‚ùå Dataset final contiene duplicados latentes
- ‚ùå Bug persiste para futuras fases
- ‚ùå Dif√≠cil defender selecci√≥n en an√°lisis posterior

**Mitigaci√≥n parcial:**
- A√±adir campo `is_from_duplicate_symbol` al manifest
- Ponderar eventos por frecuencia de reprocesamiento
- Documentar sesgo en metadata

---

## 5. AN√ÅLISIS DE S√çMBOLOS DUPLICADOS

### 5.1 Identificaci√≥n de S√≠mbolos Afectados

**Script propuesto:** `scripts/analysis/identify_duplicate_symbols.py`

```python
#!/usr/bin/env python3
"""Identify symbols that were reprocessed and caused duplicates"""

import polars as pl
from pathlib import Path

def identify_duplicate_symbols(enriched_file):
    """Identify which symbols have duplicate events"""

    df = pl.read_parquet(enriched_file)

    # Find all duplicate groups
    duplicates = df.group_by(['symbol', 'timestamp', 'event_type']).agg([
        pl.len().alias('count'),
        pl.col('enriched_at').min().alias('first_processed'),
        pl.col('enriched_at').max().alias('last_processed')
    ]).filter(pl.col('count') > 1)

    # Group by symbol to see which symbols have duplicates
    symbols_with_dups = duplicates.group_by('symbol').agg([
        pl.len().alias('duplicate_groups'),
        pl.col('count').sum().alias('total_duplicate_events')
    ]).sort('total_duplicate_events', descending=True)

    print(f"Symbols with duplicates: {len(symbols_with_dups)}")
    print(f"Total duplicate groups: {duplicates.height:,}")
    print("\nTop 20 symbols by duplicate events:")
    print(symbols_with_dups.head(20))

    return symbols_with_dups

if __name__ == "__main__":
    enriched_file = "processed/events/events_intraday_enriched_20251013_210559.parquet"
    symbols_with_dups = identify_duplicate_symbols(enriched_file)

    # Export for analysis
    symbols_with_dups.write_csv("analysis/symbols_with_duplicates_20251014.csv")
```

### 5.2 Impacto por S√≠mbolo

**Preguntas a responder:**
1. ¬øCu√°ntos s√≠mbolos fueron reprocesados? (estimado: 45-96 s√≠mbolos)
2. ¬øTienen m√°s eventos que s√≠mbolos no-duplicados?
3. ¬øEst√°n sobre-representados en manifest CORE?

**An√°lisis propuesto:**
```python
# Compare events/symbol for duplicate vs non-duplicate symbols
df_manifest = pl.read_parquet("manifest_core_20251014.parquet")

# Join with duplicate symbols list
df_analysis = df_manifest.join(
    symbols_with_dups,
    on='symbol',
    how='left'
)

# Compare distributions
duplicate_symbols = df_analysis.filter(pl.col('duplicate_groups').is_not_null())
clean_symbols = df_analysis.filter(pl.col('duplicate_groups').is_null())

print(f"Manifest CORE - Duplicate symbols:")
print(f"  Count: {duplicate_symbols['symbol'].n_unique()}")
print(f"  Events: {len(duplicate_symbols)} ({len(duplicate_symbols)/len(df_manifest)*100:.1f}%)")
print(f"  Avg events/symbol: {len(duplicate_symbols)/duplicate_symbols['symbol'].n_unique():.1f}")

print(f"\nManifest CORE - Clean symbols:")
print(f"  Count: {clean_symbols['symbol'].n_unique()}")
print(f"  Events: {len(clean_symbols)} ({len(clean_symbols)/len(df_manifest)*100:.1f}%)")
print(f"  Avg events/symbol: {len(clean_symbols)/clean_symbols['symbol'].n_unique():.1f}")
```

---

## 6. RECOMENDACI√ìN FINAL

### Decisi√≥n: **PAUSAR Y CORREGIR** (Opci√≥n 1)

**Justificaci√≥n:**

1. **Integridad del dataset:**
   - Dataset limpio es cr√≠tico para an√°lisis cient√≠fico
   - Sesgo de sobre-representaci√≥n invalida conclusiones estad√≠sticas
   - Reproducibilidad requiere proceso sin errores

2. **Eficiencia de recursos:**
   - Ahorro neto: 32h + 12 GB - 5h delay = 27h ganancia
   - API calls ahorrados: 9,600 requests (~$50-100 valor)
   - Evita re-procesamiento posterior

3. **Aprendizaje:**
   - Corregir bug previene futuros problemas en FASE 2.6, 2.7, etc.
   - Validaci√≥n cruzada checkpoint ‚Üê ‚Üí shards es best practice
   - Testing de orchestrator bajo conflictos m√∫ltiples

4. **Timeline aceptable:**
   - 5 d√≠as de delay es manejable
   - Dataset final vale la espera
   - PM wave actual solo tiene 21/1,452 eventos (1.4% progreso), m√≠nima p√©rdida

---

## 7. ACCI√ìN INMEDIATA

### Paso 1: Pausar PM Wave

```bash
# Find process
ps aux | grep download_trades_quotes | grep -v grep

# Kill process
kill -9 [PID]

# Verify stopped
ps aux | grep download_trades_quotes | grep -v grep
```

### Paso 2: Analizar S√≠mbolos Duplicados

```bash
python scripts/analysis/identify_duplicate_symbols.py
```

### Paso 3: Revisar y Corregir Orchestrator

```bash
# Backup current version
cp ultra_robust_orchestrator.py ultra_robust_orchestrator_buggy.py

# Apply fix (checkpoint validation)
# ... edit orchestrator ...

# Test with single symbol
python ultra_robust_orchestrator_fixed.py --test-mode --symbols 1
```

---

**Autor:** Claude (Anthropic)
**Fecha:** 2025-10-14 12:30 UTC
**Prioridad:** üî¥ CR√çTICA
**Acci√≥n requerida:** ‚úÖ S√ç - Pausar FASE 3.2 y corregir FASE 2.5
