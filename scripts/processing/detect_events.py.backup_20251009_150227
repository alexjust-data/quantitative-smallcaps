"""
Event Detection with Triple-Gate Logic

Detects trading events using:
- Branch 1: (Gap ≥ 10% AND RVOL ≥ 3)
- Branch 2: (ATR% ≥ p95 AND RVOL ≥ 2.5)
- Filter: DollarVolume ≥ $2M
- Additional: SSR flag, premarket volume filter

Usage:
    python scripts/processing/detect_events.py --use-percentiles
    python scripts/processing/detect_events.py --symbols AAPL TSLA --use-percentiles
"""

import sys
from pathlib import Path
from datetime import datetime, timezone, timedelta
import argparse

import polars as pl
import numpy as np
import yaml
from loguru import logger

# Add project root to path
PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(PROJECT_ROOT))


def load_config():
    """Load configuration from config.yaml"""
    config_path = PROJECT_ROOT / "config" / "config.yaml"
    with open(config_path, "r") as f:
        return yaml.safe_load(f)


def compute_daily_metrics(df: pl.DataFrame, atr_window: int = 60) -> pl.DataFrame:
    """
    Compute daily metrics: gap_pct, RVOL, ATR%, dollar_volume

    Args:
        df: DataFrame with OHLCV data (must have: timestamp, open, high, low, close, volume)
        atr_window: Window for ATR calculation

    Returns:
        DataFrame with additional columns: gap_pct, rvol, atr_pct, dollar_volume
    """
    df = df.sort("timestamp")

    # Previous close
    prev_close = pl.col("close").shift(1)

    # Gap %
    gap_pct = ((pl.col("open") - prev_close) / prev_close * 100).abs()

    # Dollar volume (using close price)
    dollar_volume = pl.col("close") * pl.col("volume")

    # Average volume (20-day rolling)
    avg_volume = pl.col("volume").rolling_mean(window_size=20, min_periods=5)

    # RVOL (Relative Volume)
    rvol = pl.col("volume") / avg_volume

    # True Range
    tr = pl.max_horizontal([
        pl.col("high") - pl.col("low"),
        (pl.col("high") - prev_close).abs(),
        (pl.col("low") - prev_close).abs()
    ])

    # ATR
    atr = tr.rolling_mean(window_size=atr_window, min_periods=atr_window // 2)

    # ATR%
    atr_pct = (atr / pl.col("close") * 100)

    return df.with_columns([
        gap_pct.alias("gap_pct"),
        rvol.alias("rvol"),
        atr_pct.alias("atr_pct"),
        dollar_volume.alias("dollar_volume"),
        avg_volume.alias("avg_volume_20d"),
        atr.alias("atr")
    ])


def compute_ssr_flag(df: pl.DataFrame) -> pl.DataFrame:
    """
    Compute SSR (Short Sale Restriction) flag

    SSR activates when low <= 0.90 * prev_close (approximation)
    """
    prev_close = pl.col("close").shift(1)
    ssr_active = (pl.col("low") <= prev_close * 0.90)

    return df.with_columns(ssr_active.alias("is_ssr"))


def compute_premarket_volume(sym: str, bars_1h_dir: Path, event_ts: datetime, cfg: dict) -> float:
    """
    Compute premarket dollar volume from 1h bars using vwap * volume

    Returns:
        Premarket dollar volume, or 0 if data not available
    """
    file_1h = bars_1h_dir / f"{sym}.parquet"
    if not file_1h.exists():
        return 0.0

    try:
        df_1h = pl.read_parquet(file_1h)

        # Convert to NY timezone
        df_1h = df_1h.with_columns(
            pl.col("timestamp").dt.convert_time_zone("America/New_York").alias("ts_ny")
        )

        # Filter to target date and PM hours (7-8 AM NY)
        pm_hours = cfg["processing"]["events"]["premarket_hours_ny"]
        event_date = event_ts.date()

        df_pm = df_1h.filter(
            (pl.col("ts_ny").dt.date() == event_date) &
            (pl.col("ts_ny").dt.hour().is_in(pm_hours))
        )

        if df_pm.height == 0:
            return 0.0

        # Dollar volume using vwap
        if "vwap" in df_pm.columns:
            pm_dvol = (df_pm["vwap"] * df_pm["volume"]).sum()
        else:
            pm_dvol = (df_pm["close"] * df_pm["volume"]).sum()

        return float(pm_dvol) if pm_dvol else 0.0

    except Exception as e:
        logger.warning(f"Failed to compute PM volume for {sym} on {event_ts}: {e}")
        return 0.0


def detect_events_for_symbol(
    symbol: str,
    bars_1d_dir: Path,
    bars_1h_dir: Path,
    cfg: dict,
    use_percentiles: bool = True
) -> pl.DataFrame:
    """
    Detect events for a single symbol

    Args:
        symbol: Ticker symbol
        bars_1d_dir: Directory with daily bars
        bars_1h_dir: Directory with hourly bars (for PM volume)
        cfg: Configuration dict
        use_percentiles: Use percentile thresholds for ATR

    Returns:
        DataFrame with event flags including 'date' and 'event_id' columns
    """
    file_1d = bars_1d_dir / f"{symbol}.parquet"

    if not file_1d.exists():
        logger.warning(f"No daily bars found for {symbol}")
        return None

    # Load daily bars
    df = pl.read_parquet(file_1d)

    if df.height < cfg["processing"]["events"]["min_trading_days"]:
        logger.debug(f"Skipping {symbol}: insufficient trading days ({df.height})")
        return None

    # Compute metrics
    atr_window = cfg["processing"]["events"]["atr_pct_window_days"]
    df = compute_daily_metrics(df, atr_window=atr_window)
    df = compute_ssr_flag(df)

    # Event config
    evt_cfg = cfg["processing"]["events"]
    gap_th = float(evt_cfg["gap_pct_threshold"])
    rvol_th = float(evt_cfg["rvol_threshold"])
    rvol_alt = float(evt_cfg["rvol_threshold_alt"])
    min_dvol = float(evt_cfg["min_dollar_volume_event"])

    # ATR threshold (percentile-based with fallback)
    if use_percentiles:
        atr_pct_p = evt_cfg["atr_pct_percentile"]
        atr_valid = df["atr_pct"].drop_nulls()
        if atr_valid.len() >= 30:
            atr_th = np.nanpercentile(atr_valid.to_numpy(), atr_pct_p)
        else:
            # Fallback for insufficient data
            atr_th = 5.0
            logger.debug(f"{symbol}: Using fallback ATR threshold (insufficient data)")
    else:
        atr_th = 5.0  # fallback fixed threshold

    # Gates
    df = df.with_columns([
        (pl.col("gap_pct") >= gap_th).alias("gate_gap"),
        (pl.col("rvol") >= rvol_th).alias("gate_rvol"),
        (pl.col("rvol") >= rvol_alt).alias("gate_rvol_alt"),
        (pl.col("atr_pct") >= atr_th).alias("gate_atr"),
        (pl.col("dollar_volume") >= min_dvol).alias("gate_dv")
    ])

    # Event detection: (gap & rvol) OR (atr & rvol_alt), AND dollar_volume OK
    is_event = (
        ((pl.col("gate_gap") & pl.col("gate_rvol")) |
         (pl.col("gate_atr") & pl.col("gate_rvol_alt")))
        & pl.col("gate_dv")
    )
    df = df.with_columns(is_event.alias("is_event"))

    # Premarket filter (optional)
    if evt_cfg.get("use_hourly_premarket_filter", False):
        pm_min = float(evt_cfg.get("premarket_min_dollar_volume", 0))

        # Get events with timestamps
        ev = df.filter(pl.col("is_event")).select(["timestamp"])

        if ev.height > 0:
            # Compute pm_ok for each event
            pm_ok_list = []
            for ts in ev["timestamp"]:
                pm_vol = compute_premarket_volume(symbol, bars_1h_dir, ts, cfg)
                pm_ok_list.append(pm_vol >= pm_min)

            # Add pm_ok column to events
            ev = ev.with_columns(pl.Series("pm_ok", pm_ok_list))

            # Join back to main df (default True for non-events)
            df = df.join(ev, on="timestamp", how="left").with_columns(
                pl.col("pm_ok").fill_null(True)
            )

            # Update is_event
            df = df.with_columns(
                (pl.col("is_event") & pl.col("pm_ok")).alias("is_event")
            )

    # Add date column for easy filtering
    df = df.with_columns(
        pl.col("timestamp").dt.date().alias("date")
    )

    # Add event_id for traceability (YYYYMMDD for event days, null otherwise)
    df = df.with_columns(
        pl.when(pl.col("is_event"))
        .then(pl.col("timestamp").dt.strftime("%Y%m%d"))
        .otherwise(None)
        .alias("event_id")
    )

    # Add symbol column
    df = df.with_columns(pl.lit(symbol).alias("symbol"))

    # Select relevant columns
    df = df.select([
        "symbol",
        "date",
        "timestamp",
        "event_id",
        "open",
        "high",
        "low",
        "close",
        "volume",
        "gap_pct",
        "rvol",
        "atr_pct",
        "dollar_volume",
        "is_ssr",
        "gate_gap",
        "gate_rvol",
        "gate_rvol_alt",
        "gate_atr",
        "gate_dv",
        "is_event"
    ])

    return df


def main():
    parser = argparse.ArgumentParser(description="Detect trading events with triple-gate logic")
    parser.add_argument("--symbols", nargs="+", help="Specific symbols to process (default: all)")
    parser.add_argument("--use-percentiles", action="store_true", help="Use percentile thresholds for ATR")
    parser.add_argument("--output-dir", type=str, help="Output directory (default: processed/events)")
    args = parser.parse_args()

    # Load config
    cfg = load_config()

    # Paths
    bars_1d_dir = PROJECT_ROOT / cfg["paths"]["raw"] / "market_data" / "bars" / "1d"
    bars_1h_dir = PROJECT_ROOT / cfg["paths"]["raw"] / "market_data" / "bars" / "1h"

    output_dir = Path(args.output_dir) if args.output_dir else PROJECT_ROOT / cfg["paths"]["processed"] / "events"
    output_dir.mkdir(parents=True, exist_ok=True)

    # Get symbols
    if args.symbols:
        symbols = args.symbols
    else:
        # All symbols with daily bars
        symbols = sorted([f.stem for f in bars_1d_dir.glob("*.parquet")])

    logger.info(f"Processing {len(symbols)} symbols for event detection")
    logger.info(f"Using percentiles: {args.use_percentiles}")

    # Process each symbol
    results = []
    for i, symbol in enumerate(symbols):
        if (i + 1) % 100 == 0:
            logger.info(f"Progress: {i+1}/{len(symbols)} symbols")

        df_events = detect_events_for_symbol(
            symbol,
            bars_1d_dir,
            bars_1h_dir,
            cfg,
            use_percentiles=args.use_percentiles
        )

        if df_events is not None:
            results.append(df_events)

    # Combine results
    if results:
        df_all = pl.concat(results)

        # Save
        ts = datetime.now(timezone.utc).strftime("%Y%m%d")
        output_file = output_dir / f"events_daily_{ts}.parquet"
        df_all.write_parquet(output_file, compression="zstd")

        logger.info(f"Saved events to {output_file}")

        # Summary stats
        total_events = df_all.filter(pl.col("is_event")).height
        total_days = df_all.height
        event_rate = total_events / total_days * 100 if total_days > 0 else 0

        logger.info(f"Total events detected: {total_events:,}")
        logger.info(f"Total days: {total_days:,}")
        logger.info(f"Event rate: {event_rate:.2f}%")

        # Monthly event density summary
        monthly_summary = (df_all.filter(pl.col("is_event"))
                          .with_columns(pl.col("date").dt.strftime("%Y-%m").alias("month"))
                          .group_by("month")
                          .agg(pl.len().alias("n_events"))
                          .sort("month"))

        logger.info(f"\nMonthly event density:")
        print(monthly_summary)

        # Top symbols by event count
        top_symbols = (df_all.filter(pl.col("is_event"))
                       .group_by("symbol")
                       .agg(pl.len().alias("n_events"))
                       .sort("n_events", descending=True)
                       .head(20))

        logger.info(f"\nTop 20 symbols by event count:")
        print(top_symbols)
    else:
        logger.warning("No events detected")


if __name__ == "__main__":
    main()
